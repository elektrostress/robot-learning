---
title: "Exercise prediction - a Course Project for Practical Machine Learning"
author: "Timo"
date: "22 May 2015"
output: html_document
---
## Exploration and feature selection

```{r, results='hide', message=FALSE, warning=FALSE}
library(ggplot2);
library(caret);
library(Hmisc);
require(randomForest);
```

Load in the required data set:
```{r}
set.seed(12345)
pml_training <- read.csv("/Users/timo/Dropbox (Personal)/Practical Machine Learning/project/data/pml-training.csv", na.strings=c("NA","", "#DIV/0!"), header=TRUE)
```

Create a holdout validation set, as we are interested in the out-of-sample error after fitting and training.
```{r}
inValidation = createDataPartition(pml_training$classe, p = 0.10)[[1]]
validation = pml_training[inValidation,]
trainingWhole =  pml_training[-inValidation,]
```

Remove columns with over 80% NA's, as it is unlikely that these will help prediction.
```{r}
trainingWhole <- trainingWhole[,(which((colSums(is.na(pml_training)) / NROW(pml_training)) < .80))]
```

This plot shows that each user performed the 5 variations in alphabetic order. This information is (partially) present in num_window, raw_timestamp_part_1 and raw_timestamp_part_2 but it is best shown by the cvtd_timestamp.
```{r, echo=FALSE}
# todo: fix cvtd axis labels
ggplot(trainingWhole, aes(x=cvtd_timestamp,y=classe,colour=user_name)) + geom_point()
```

With a simple random forest setup with only 10% of the dataset to train on (initially set for speed) we can already obtain an  accuracy of 99% on the test set. 
```{r, message=FALSE, cache=TRUE}
# small test for speed (p=0.10)
inTrain = createDataPartition(trainingWhole$classe, p = 0.10)[[1]]
testTimestampTrain = trainingWhole[inTrain,]
testTimestampTest = trainingWhole[-inTrain,]
testTimestampTrain <- testTimestampTrain[,(names(testTimestampTrain) %in% c("user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "num_window", "classe"))]

trainCtrl <- trainControl(method = "cv", 
                     number=5,
                     verboseIter = TRUE,
                     savePredictions = TRUE)

testmodelFit <- train(classe~.,data=testTimestampTrain,method="rf",
                trControl=trainCtrl,
                prox=TRUE,allowParallel=TRUE)

confOverall <- confusionMatrix(testTimestampTest$classe, predict(testmodelFit, testTimestampTest))
# return the accuracy on our test set
print(confOverall$overall)
# confusionmatrix 5x5
print(round(prop.table(confOverall$table,2),2))
```


However, we're interested in how the actual exercise measurements are related to the classe variable. Thus I remove "user_name" "raw_timestamp_part_1" "raw_timestamp_part_2" "cvtd_timestamp" "num_window" as these will not have any predictive power in future instances, and result in overfitting on our dataset.

```{r}
# delete the irrelevant columns
trainingWhole <- trainingWhole[,!(names(trainingWhole) %in% c("user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "num_window"))]
```

## Training
Now, split the remaining training set in a testing and training set.
```{r, cache=TRUE}
# Usually, we would select p = 0.70, but for speed and easy replicability I've used 0.25 as the training set.
inTrain = createDataPartition(trainingWhole$classe, p = 0.25)[[1]]
training = trainingWhole[inTrain,]
testing =  trainingWhole[-inTrain,]
```

In order to train the model I used cross-validation with 5 folds (initially, also for speed).
```{r, cache=TRUE, message=FALSE}
trainCtrl <- trainControl(method = "cv", 
                     number=5,
                     verboseIter = TRUE,
                     savePredictions = TRUE)

modelFit <- train(classe~.,data=training,method="rf",
                trControl=trainCtrl,
                prox=TRUE,allowParallel=TRUE)

saveRDS(modelFit, "finaltrain_modelfit.rds")

finalconfOverall <- confusionMatrix(testing$classe, predict(modelFit, testing))
# return the accuracy on our test set
print(finalconfOverall)
# output a confusionmatrix 5x5
print(round(prop.table(finalconfOverall$table,2),2))
```

In order to estimate the out-of-sample error we use the previously created validation set.
```{r, cache=TRUE}
confValid <- confusionMatrix(validation$classe, predict(modelFit, validation))
# return the accuracy on our test set
print(confValid)
# output a confusionmatrix 5x5
print(round(prop.table(confValid$table,2),2))
```

## Out of sample error

The expected out of sample error for this dataset is thus 0 (1-0.9981). This is, however, extremely low for a real life situation and one should interpret these results with caution.
