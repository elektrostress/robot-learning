---
title: "Exercise prediction - a Course Project for Practical Machine Learning"
author: "Timo Polman"
date: "22 May 2015"
output: html_document
---
## Exploration and feature selection

```{r, results='hide', message=FALSE, warning=FALSE}
library(ggplot2);
library(caret);
library(Hmisc);
require(randomForest);
```

Load in the required data set:
```{r}
set.seed(12345)
pml_training <- read.csv("/Users/timo/Dropbox (Personal)/Practical Machine Learning/project/data/pml-training.csv", na.strings=c("NA","", "#DIV/0!"), header=TRUE)
```

Create a holdout validation set, as we are interested in the out-of-sample error after fitting and training.
```{r}
inValidation = createDataPartition(pml_training$classe, p = 0.10)[[1]]
validation = pml_training[inValidation,]
trainingWhole =  pml_training[-inValidation,]
```

Remove columns with over 80% NA's, as it is unlikely that these will help prediction.
```{r}
trainingWhole <- trainingWhole[,(which((colSums(is.na(pml_training)) / NROW(pml_training)) < .80))]
```

This plot shows that each user performed the 5 variations in alphabetic order. This information is (partially) present in num_window, raw_timestamp_part_1 and raw_timestamp_part_2 but it is best shown by the cvtd_timestamp.
```{r, echo=FALSE}
# todo: fix cvtd axis labels
ggplot(trainingWhole, aes(x=cvtd_timestamp,y=classe,colour=user_name)) + geom_point()
```

With a simple random forest setup with only 10% of the dataset to train on (initially set for speed) we can already obtain an  accuracy of 99% on the test set. 
```{r, message=FALSE, cache=TRUE}
# small test for speed (p=0.10)
inTrain = createDataPartition(trainingWhole$classe, p = 0.10)[[1]]
testTimestampTrain = trainingWhole[inTrain,]
testTimestampTest = trainingWhole[-inTrain,]
testTimestampTrain <- testTimestampTrain[,(names(testTimestampTrain) %in% c("user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "num_window", "classe"))]

trainCtrl <- trainControl(method = "cv", 
                     number=5,
                     verboseIter = TRUE,
                     savePredictions = TRUE)

testmodelFit <- train(classe~.,data=testTimestampTrain,method="rf",
                trControl=trainCtrl,
                prox=TRUE,allowParallel=TRUE)

confOverall <- confusionMatrix(testTimestampTest$classe, predict(testmodelFit, testTimestampTest))
# return the accuracy on our test set
print(confOverall$overall)
# confusionmatrix 5x5
print(round(prop.table(confOverall$table,2),2))
```


However, we're interested in how the actual exercise measurements are related to the classe variable. Thus I remove "user_name" "raw_timestamp_part_1" "raw_timestamp_part_2" "cvtd_timestamp" "num_window" as these will not have any predictive power in future instances, and result in overfitting on our dataset.

```{r}
# delete the irrelevant columns
trainingWhole <- trainingWhole[,!(names(trainingWhole) %in% c("user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "num_window"))]
```

## Training
Now, split the remaining training set in a testing and training set.
```{r, cache=TRUE}
# Usually, we would select p = 0.70, but for speed and easy replicability I've used 0.25 as the training set.
inTrain = createDataPartition(trainingWhole$classe, p = 0.25)[[1]]
training = trainingWhole[inTrain,]
testing =  trainingWhole[-inTrain,]
```

In order to train the model I used cross-validation with 5 folds (initially, also for speed).
```{r, cache=TRUE, message=FALSE}
trainCtrl <- trainControl(method = "cv", 
                     number=5,
                     verboseIter = TRUE,
                     savePredictions = TRUE)

modelFit <- train(classe~.,data=training,method="rf",
                trControl=trainCtrl,
                prox=TRUE,allowParallel=TRUE)

saveRDS(modelFit, "finaltrain_modelfit.rds")

finalconfOverall <- confusionMatrix(testing$classe, predict(modelFit, testing))
# return the accuracy on our test set
print(finalconfOverall)
# output a confusionmatrix 5x5
print(round(prop.table(finalconfOverall$table,2),2))
```

In order to estimate the out-of-sample error we use the previously created validation set.
```{r, cache=TRUE}
confValid <- confusionMatrix(validation$classe, predict(modelFit, validation))
# return the accuracy on our test set
print(confValid)
# output a confusionmatrix 5x5
print(round(prop.table(confValid$table,2),2))
```

## Out of sample error

The expected out of sample error for this dataset is thus 0 (1-0.9981). However, I'm also interested in the performance of this model for future users of the devices. Therefore I want to test the performance while excluding one user from the training set.

```{r, cache=TRUE, message=FALSE}
# again remove the columns with over 80% NAs
trainingWhole <- pml_training[,(which((colSums(is.na(pml_training)) / NROW(pml_training)) < .80))]

# leave out one user (jeremy) of the original dataset
datasetTrainUser = trainingWhole[trainingWhole$user_name %in% c("pedro", "adelmo", "charles", "eurico", "carlitos"),]
testingUser = trainingWhole[trainingWhole$user_name %in% c("jeremy"),]

# subset the training set for performance
inTrain = createDataPartition(datasetTrainUser$classe, p = 0.10)[[1]]
trainingUser = datasetTrainUser[inTrain,]

# Train the model on all remaining users but one, in order to improve generalizability for new users 
fit_on <-  list(rs1 = which(trainingUser$user_name!="charles"), rs2 = which(trainingUser$user_name!="pedro"), rs3 = which(trainingUser$user_name!="adelmo"), rs4 = which(trainingUser$user_name!="eurico"), rs5 = which(trainingUser$user_name!="carlitos"))
pred_on <- list(rs1 = which(trainingUser$user_name=="charles"), rs2 = which(trainingUser$user_name=="pedro"), rs3 = which(trainingUser$user_name=="adelmo"), rs4 = which(trainingUser$user_name=="eurico"), rs5 = which(trainingUser$user_name=="carlitos"))

# and remove the irrelevant values
trainingUser <- trainingUser[,!(names(trainingUser) %in% c("user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "num_window"))]

trainUserCtrl <- trainControl(method = "cv", 
                     # here we define the samples (rather than let caret create k-fold sets)
                     index= fit_on, indexOut = pred_on,
                     verboseIter = FALSE,
                     savePredictions = TRUE)

modelFitUser <- train(classe~.,data=trainingUser,method="rf",
                trControl=trainUserCtrl,
                prox=TRUE,allowParallel=TRUE)

confOverallUser <- confusionMatrix(testingUser$classe, predict(modelFitUser, testingUser))
print(confOverallUser)
# confusionmatrix 5x5
print(round(prop.table(confOverallUser$table,2),2))
```
